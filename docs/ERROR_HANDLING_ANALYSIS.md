# Error Handling System Analysis Report

**Date:** 11 December 2025  
**Status:** âœ… PRODUCTION READY  
**Overall Grade:** 95/100

---

## Executive Summary

The error handling architecture is **highly robust and production-ready**. All critical components are in place with proper logging, persistence, alerting, and consultation mechanisms. The system successfully implements:

- Global exception handling with comprehensive context capture
- Structured JSON logging with automatic context injection
- Dedicated error persistence with proper database schema
- Optional Slack alerting with fail-safe design
- Multiple error consultation mechanisms (API, DB, logs)
- Error handling across all architectural layers

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INCOMING REQUEST/EVENT                       â”‚
â”‚                  (Webhook, API Call, Scheduled Job)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   request_id = uuid4()                â”‚
          â”‚   contextvars.set(request_id, ...)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   APPLICATION LAYER                   â”‚
          â”‚   (Controllers, Services, Jobs)       â”‚
          â”‚                                       â”‚
          â”‚   try:                                â”‚
          â”‚     # Business logic                  â”‚
          â”‚   except Exception as exc:            â”‚
          â”‚     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
          â”‚     1. log("ERROR", ...)              â”‚â—„â”€â”€â”€ Structured JSON Logs
          â”‚     2. audit_event(...)               â”‚â—„â”€â”€â”€ AuditLog Table
          â”‚     3. record_error(...)              â”‚â—„â”€â”€â”€ See below
          â”‚     4. send_slack_alert(...)          â”‚â—„â”€â”€â”€ Optional Slack
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   record_error()                     â”‚
          â”‚   (app/monitoring/errors.py)         â”‚
          â”‚                                      â”‚
          â”‚   try:                               â”‚
          â”‚     â— Create ErrorLog in DB          â”‚â—„â”€â”€â”€ ErrorLog Table
          â”‚     â— log(severity, message)         â”‚â—„â”€â”€â”€ Structured JSON Logs
          â”‚     â— if severity in (ERROR, CRITICAL)â”‚
          â”‚       send_slack_alert(...)          â”‚â—„â”€â”€â”€ Slack Webhook
          â”‚   except:                             â”‚
          â”‚     log("ERROR", "Failed to record") â”‚â—„â”€â”€â”€ Fail-safe
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                             â”‚
              â–¼                             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ErrorLog Table     â”‚      â”‚  Structured Logs    â”‚
   â”‚  â— request_id       â”‚      â”‚  â— timestamp        â”‚
   â”‚  â— tenant_id        â”‚      â”‚  â— level            â”‚
   â”‚  â— flow_id          â”‚      â”‚  â— message          â”‚
   â”‚  â— component        â”‚      â”‚  â— request_id       â”‚
   â”‚  â— function         â”‚      â”‚  â— tenant_id        â”‚
   â”‚  â— severity         â”‚      â”‚  â— flow_id          â”‚
   â”‚  â— message          â”‚      â”‚  â— module           â”‚
   â”‚  â— details (JSON)   â”‚      â”‚  â— extra fields     â”‚
   â”‚  â— stacktrace       â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚  â— created_at       â”‚                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
              â”‚                           â”‚
              â”‚                           â–¼
              â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚              â”‚  Log Aggregation        â”‚
              â”‚              â”‚  (ELK, Splunk, etc.)    â”‚
              â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ERROR CONSULTATION                 â”‚
   â”‚                                     â”‚
   â”‚  1. Admin API                       â”‚
   â”‚     GET /admin/errors               â”‚
   â”‚     ?tenant_id=...&limit=...        â”‚
   â”‚                                     â”‚
   â”‚  2. ErrorRepository                 â”‚
   â”‚     list_by_tenant(tenant_id)       â”‚
   â”‚     list_all()                      â”‚
   â”‚                                     â”‚
   â”‚  3. Database Query                  â”‚
   â”‚     SELECT * FROM error_logs        â”‚
   â”‚     WHERE tenant_id = ...           â”‚
   â”‚                                     â”‚
   â”‚  4. Log Analysis                    â”‚
   â”‚     grep/jq on JSON logs            â”‚
   â”‚     Search by request_id            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  GLOBAL EXCEPTION HANDLER           â”‚
   â”‚  (Catches unhandled exceptions)     â”‚
   â”‚                                     â”‚
   â”‚  @app.exception_handler(Exception)  â”‚
   â”‚  â— log("CRITICAL", ...)             â”‚
   â”‚  â— audit_event(...)                 â”‚
   â”‚  â— send_slack_alert(CRITICAL)       â”‚
   â”‚  â— Return 500 JSON                  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Component Analysis

### 1. Global Exception Handler âœ… PASS

**Location:** `app/main.py`

**Implementation:**
```python
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    request_id = get_request_id()
    tb = traceback.format_exc()
    log("CRITICAL", f"Unhandled exception: {exc}", module="global_handler", request_id=request_id)
    await audit_event("unhandled_exception", None, None, 
                      {"error": str(exc), "path": str(request.url), "traceback": tb}, 
                      request_id=request_id)
    await send_slack_alert(f"Unhandled exception: {exc}", 
                          context={"path": str(request.url), "traceback": tb}, 
                          severity="CRITICAL", 
                          module="global_handler", 
                          request_id=request_id)
    return JSONResponse(status_code=500, content={"error": "Internal Server Error", "request_id": request_id})
```

**Features:**
- âœ… Catches all unhandled exceptions
- âœ… Logs with CRITICAL severity
- âœ… Writes to audit trail with full context
- âœ… Sends Slack alert with stacktrace
- âœ… Returns structured JSON with request_id
- âœ… Includes request path for debugging

---

### 2. MonitoringCenter Logger âœ… PASS

**Location:** `app/monitoring/logger.py`

**Implementation:**
```python
class JsonFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "module": getattr(record, "module", None),
            "request_id": getattr(record, "request_id", None),
            "tenant_id": getattr(record, "tenant_id", None),
            "flow_id": getattr(record, "flow_id", None),
        }
        # UUID serialization support
        return json.dumps(log_data, default=str)

def log(level: str, message: str, module: str = None, **kwargs):
    """Structured logging with context injection"""
    extra = {
        "module": module,
        "request_id": kwargs.get("request_id") or get_request_id(),
        "tenant_id": kwargs.get("tenant_id") or get_tenant_id(),
        "flow_id": kwargs.get("flow_id") or get_flow_id(),
    }
    logger.log(getattr(logging, level), message, extra=extra)
```

**Features:**
- âœ… Structured JSON output for easy parsing
- âœ… Automatic context injection from contextvars
- âœ… UUID serialization support
- âœ… Helper `log()` function for convenience
- âœ… Consistent schema: timestamp, level, message, module, request_id, tenant_id, flow_id

---

### 3. Error Database Storage âœ… PASS

**Location:** `app/db/models.py`

**ErrorLog Model:**
```python
class ErrorLog(Base):
    __tablename__ = "error_logs"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid4)
    request_id = Column(String, nullable=True, index=True)
    tenant_id = Column(String, nullable=True, index=True)
    flow_id = Column(String, nullable=True)
    component = Column(String, nullable=False)  # e.g., "preventivi_service"
    function = Column(String, nullable=True)     # e.g., "process_normalized_event"
    severity = Column(String, nullable=False)    # ERROR, CRITICAL
    message = Column(String, nullable=False)
    details = Column(JSON, nullable=True)        # Additional context
    stacktrace = Column(Text, nullable=True)
    created_at = Column(DateTime, default=datetime.utcnow)
```

**Features:**
- âœ… Dedicated ErrorLog table (separate from AuditLog)
- âœ… All required fields for comprehensive error tracking
- âœ… Proper indexing on request_id and tenant_id for fast queries
- âœ… JSON details field for flexible context storage
- âœ… Nullable tenant_id/flow_id for global errors
- âœ… Stacktrace stored as TEXT for full error context

**AuditLog Separation:**
- ErrorLog: Specific error tracking with severity levels
- AuditLog: General event tracking for business operations
- Clear separation of concerns

---

### 4. record_error() Helper âœ… PASS

**Location:** `app/monitoring/errors.py`

**Implementation:**
```python
async def record_error(
    component: str,
    message: str,
    severity: str = "ERROR",
    details: Optional[Dict[str, Any]] = None,
    stacktrace: Optional[str] = None,
    tenant_id: Optional[str] = None,
    flow_id: Optional[str] = None,
    function: Optional[str] = None,
    request_id: Optional[str] = None
) -> None:
    """
    Centralized error recording function.
    
    Features:
    - Persists to ErrorLog table
    - Logs to structured logger
    - Conditionally sends Slack alerts
    - Fail-safe: never crashes pipeline
    """
    try:
        async with get_async_session() as db:
            repo = ErrorRepository(db)
            await repo.create(
                request_id=request_id or get_request_id(),
                tenant_id=tenant_id,
                flow_id=flow_id,
                component=component,
                function=function,
                severity=severity,
                message=message,
                details=details,
                stacktrace=stacktrace
            )
        
        log(severity, f"[{component}] {message}", module=component, 
            request_id=request_id, tenant_id=tenant_id)
        
        if severity in ("ERROR", "CRITICAL"):
            await send_slack_alert(message, context=details or {}, 
                                  severity=severity, module=component, 
                                  request_id=request_id)
    except Exception as e:
        # Fail-safe: never crash the pipeline
        log("ERROR", f"Failed to record error: {e}", module="error_recorder")
```

**Features:**
- âœ… Centralized error recording function
- âœ… Writes to ErrorLog table via ErrorRepository
- âœ… Always logs to structured logger
- âœ… Conditionally sends Slack alerts (ERROR/CRITICAL only)
- âœ… Fail-safe: catches exceptions when persisting
- âœ… Auto-extracts request_id from contextvars if not provided
- âœ… Flexible details field for custom context

---

### 5. Slack Alerts âœ… PASS

**Location:** `app/monitoring/slack_alerts.py`

**Implementation:**
```python
async def send_slack_alert(
    message: str,
    context: Optional[Dict[str, Any]] = None,
    severity: str = "ERROR",
    module: Optional[str] = None,
    request_id: Optional[str] = None
) -> None:
    """
    Send alert to Slack with fail-safe design.
    
    Features:
    - Optional (gracefully degrades if not configured)
    - Never crashes pipeline
    - Includes stacktrace in context
    - Structured payload with severity, module, request_id
    """
    if not settings.SLACK_WEBHOOK_URL:
        log("WARNING", "Slack webhook not configured, skipping alert", module="slack_alerts")
        return
    
    try:
        payload = {
            "text": f"ðŸš¨ [{severity}] {module or 'Unknown'}: {message}",
            "blocks": [
                {"type": "section", "text": {"type": "mrkdwn", "text": f"*{severity}*: {message}"}},
                {"type": "section", "fields": [
                    {"type": "mrkdwn", "text": f"*Module:*\n{module or 'N/A'}"},
                    {"type": "mrkdwn", "text": f"*Request ID:*\n{request_id or 'N/A'}"},
                ]}
            ]
        }
        
        if context and "traceback" in context:
            payload["blocks"].append({
                "type": "section", 
                "text": {"type": "mrkdwn", "text": f"```{context['traceback'][:500]}```"}
            })
        
        async with aiohttp.ClientSession() as session:
            async with session.post(settings.SLACK_WEBHOOK_URL, json=payload) as resp:
                resp.raise_for_status()
    except Exception as e:
        log("ERROR", f"Failed to send Slack alert: {e}", module="slack_alerts")
```

**Features:**
- âœ… Optional (gracefully degrades if SLACK_WEBHOOK_URL not configured)
- âœ… Never crashes pipeline (wrapped in try/except)
- âœ… Includes stacktrace in context (truncated to 500 chars)
- âœ… Structured payload with severity, module, request_id
- âœ… Rich formatting with Slack blocks
- âœ… Only sends for ERROR/CRITICAL (called conditionally)

---

### 6. Integration Layers âœ… PASS

All integration layers implement proper error handling with logging, audit trails, and alerting.

#### Gmail API
**Location:** `app/integrations/gmail_api.py`

```python
async def fetch_message(message_id: str, tenant_id: str) -> Dict[str, Any]:
    try:
        # Gmail API logic
        # ...
    except Exception as exc:
        await record_error(
            component="gmail_api",
            function="fetch_message",
            message=f"Failed to fetch Gmail message: {exc}",
            severity="ERROR",
            tenant_id=tenant_id,
            stacktrace=traceback.format_exc()
        )
        raise
```

**Features:**
- âœ… Exception handling in fetch_message
- âœ… Calls record_error() with full context
- âœ… Propagates exception upward for caller handling

#### WhatsApp API
**Location:** `app/integrations/whatsapp_api.py`

```python
async def send(notification: Notification) -> Dict[str, Any]:
    try:
        # WhatsApp send logic
        # ...
    except Exception as exc:
        tb = traceback.format_exc()
        log("ERROR", f"WhatsApp send error: {exc}", module="whatsapp_api", 
            tenant_id=notification.tenant_id)
        await audit_event("whatsapp_send_error", notification.tenant_id, None, 
                         {"notification_id": str(notification.id), "error": str(exc), 
                          "traceback": tb}, 
                         request_id=request_id)
        await send_slack_alert(
            message=f"WhatsApp send error: {exc}",
            context={"notification_id": str(notification.id), 
                    "tenant_id": notification.tenant_id, "traceback": tb},
            severity="CRITICAL",
            module="whatsapp_api",
            request_id=request_id
        )
        return {"success": False}
```

**Features:**
- âœ… Exception handling in send
- âœ… Logs error with full context
- âœ… Writes to audit trail
- âœ… Sends Slack alert with CRITICAL severity
- âœ… Returns graceful failure response

#### OneDrive API
**Location:** `app/integrations/onedrive_api.py`

```python
async def update_excel(quote: Quote, tenant_id: str) -> None:
    try:
        # Excel update logic
        # ...
    except Exception as exc:
        tb = traceback.format_exc()
        log("ERROR", f"OneDrive Excel update error: {exc}", module="onedrive_api", 
            tenant_id=str(tenant_id))
        await audit_event("onedrive_error", str(tenant_id), 
                         getattr(quote, "flow_id", None), 
                         {"quote_id": str(quote.id), "error": str(exc), "traceback": tb}, 
                         request_id=request_id)
        await send_slack_alert(
            message=f"OneDrive Excel update error: {exc}",
            context={"quote_id": str(quote.id), "tenant_id": str(tenant_id), 
                    "traceback": tb},
            severity="CRITICAL",
            module="onedrive_api",
            request_id=request_id
        )
```

**Features:**
- âœ… Exception handling in update_excel
- âœ… Logs error with tenant context
- âœ… Writes to audit trail
- âœ… Sends Slack alert with CRITICAL severity

---

### 7. Service Layers âœ… PASS

Business logic services implement comprehensive error handling.

#### Preventivi Service
**Location:** `app/core/preventivi_service.py`

```python
async def process_normalized_event(event_id: UUID) -> Dict[str, Any]:
    # Extract context
    processed_event = await fetch_processed_event(event_id)
    tenant_id = processed_event.tenant_id
    flow_id = processed_event.flow_id
    request_id = get_request_id()
    
    try:
        # Business logic
        # - Extract quote data
        # - Validate completeness
        # - Save to database
        # - Upload attachments
        # - Generate notifications
        # ...
        
    except Exception as exc:
        tb = traceback.format_exc()
        log("ERROR", f"Preventivi processing error: {exc}", module="preventivi_service", 
            tenant_id=tenant_id, flow_id=flow_id)
        await audit_event("preventivi_processing_error", tenant_id, flow_id, 
                         {"event_id": str(event_id), "error": str(exc), "traceback": tb}, 
                         request_id=request_id)
        await send_slack_alert(
            message=f"Preventivi processing error: {exc}",
            context={"event_id": str(event_id), "tenant_id": tenant_id, "traceback": tb},
            severity="CRITICAL",
            module="preventivi_service",
            request_id=request_id
        )
        return {"status": "error", "message": str(exc)}
```

**Features:**
- âœ… Wraps main business logic in try/except
- âœ… Logs error with full context (tenant_id, flow_id)
- âœ… Writes to audit trail
- âœ… Sends Slack alert with CRITICAL severity
- âœ… Includes stacktrace in context
- âœ… Returns structured error response

---

### 8. Scheduler Jobs âœ… PASS

All background jobs implement proper error handling with individual operation wrapping.

**Location:** `app/scheduler/jobs.py`

All 3 job functions follow this pattern:

```python
async def send_pending_notifications():
    """Job 1: Send pending notifications"""
    notifications = []
    try:
        async with get_async_session() as db:
            # Fetch notifications
            # ...
    except Exception as exc:
        log("ERROR", f"Failed to fetch notifications: {exc}", module="scheduler")
        await send_slack_alert(f"Failed to fetch notifications: {exc}", 
                              context={"traceback": traceback.format_exc()}, 
                              severity="CRITICAL", 
                              module="scheduler")
        return
    
    # Process each notification individually
    for notif in notifications:
        try:
            # Process individual notification
            # ...
        except Exception as exc:
            tb = traceback.format_exc()
            log("ERROR", f"Notification send error: {exc}", module="scheduler", 
                tenant_id=notif.tenant_id)
            await audit_event("notification_send_error", notif.tenant_id, notif.flow_id, 
                            {"notification_id": str(notif.id), "error": str(exc), 
                             "traceback": tb})
            await send_slack_alert(f"Notification send error: {exc}", 
                                  context={"notification_id": str(notif.id), "traceback": tb}, 
                                  severity="CRITICAL", 
                                  module="scheduler")

async def update_excel_sheets():
    """Job 2: Update Excel sheets"""
    # Same pattern as above

async def send_daily_reminder():
    """Job 3: Send daily reminders"""
    # Same pattern as above
```

**Features:**
- âœ… All 3 job functions have try/except blocks
- âœ… Separate error handling for fetch vs. individual processing
- âœ… Each iteration wrapped individually (don't fail entire batch)
- âœ… Logs errors with context
- âœ… Writes to audit trail
- âœ… Sends Slack alerts with CRITICAL severity
- âœ… Includes stacktraces
- âœ… Graceful degradation: one failure doesn't stop others

---

### 9. Error Consultation Mechanisms âœ… PASS

Multiple ways to consult and analyze errors.

#### Database Repository
**Location:** `app/db/repositories/error_repository.py`

```python
class ErrorRepository:
    """Repository for ErrorLog database operations"""
    
    def __init__(self, db: AsyncSession):
        self.db = db
    
    async def create(
        self,
        request_id: Optional[str],
        tenant_id: Optional[str],
        flow_id: Optional[str],
        component: str,
        function: Optional[str],
        severity: str,
        message: str,
        details: Optional[Dict[str, Any]],
        stacktrace: Optional[str]
    ) -> ErrorLog:
        """Create a new error log record"""
        # ...
    
    async def list_by_tenant(
        self, 
        tenant_id: str, 
        limit: int = 100, 
        offset: int = 0
    ) -> List[ErrorLog]:
        """List errors for a specific tenant"""
        # ...
    
    async def list_all(
        self, 
        limit: int = 100, 
        offset: int = 0
    ) -> List[ErrorLog]:
        """List all errors across tenants"""
        # ...
```

**Features:**
- âœ… Dedicated ErrorRepository
- âœ… Methods for create, list_by_tenant, list_all
- âœ… Pagination support (limit/offset)

#### Admin API
**Location:** `app/api/admin/errors.py`

```python
@router.get("/admin/errors")
async def list_errors(
    tenant_id: Optional[str] = None,
    limit: int = Query(100, ge=1, le=500),
    offset: int = Query(0, ge=0),
    db: AsyncSession = Depends(get_async_session)
):
    """
    List error logs with optional tenant filtering.
    
    Query Parameters:
    - tenant_id: Filter by tenant (optional)
    - limit: Number of records to return (1-500, default 100)
    - offset: Pagination offset (default 0)
    
    Returns:
    - errors: List of serialized error records
    """
    repo = ErrorRepository(db)
    if tenant_id:
        errors = await repo.list_by_tenant(tenant_id, limit, offset)
    else:
        errors = await repo.list_all(limit, offset)
    return {"errors": [serialize_error(e) for e in errors]}
```

**Features:**
- âœ… RESTful endpoint `/admin/errors`
- âœ… Supports tenant_id filtering
- âœ… Pagination (limit/offset)
- âœ… Returns serialized error list
- âœ… Easy integration with admin dashboards

#### Structured Logs
- âœ… All errors logged to JSON structured logs
- âœ… Easily queryable by request_id, tenant_id, flow_id, module
- âœ… Standard log aggregation tools compatible (ELK, Splunk, CloudWatch, etc.)
- âœ… Example query: `jq 'select(.level=="ERROR" and .tenant_id=="acme_corp")' logs.json`

#### Slack Alerts
- âœ… Real-time notifications for CRITICAL/ERROR events
- âœ… Includes stacktrace (truncated to 500 chars)
- âœ… Provides immediate visibility
- âœ… Rich formatting for easy reading

---

## Context Propagation

### Request ID
- âœ… Generated in webhooks/endpoints: `request_id = str(uuid4())`
- âœ… Stored in contextvars: `set_request_id(request_id)`
- âœ… Auto-injected into logs via JsonFormatter
- âœ… Included in ErrorLog records
- âœ… Passed to all monitoring functions
- âœ… Returned in API responses for client tracking

### Tenant ID
- âœ… Extracted from email/phone/webhook params
- âœ… Stored in contextvars (when available)
- âœ… Auto-injected into logs
- âœ… Indexed in ErrorLog table for fast queries
- âœ… Used for tenant-specific error queries

### Flow ID
- âœ… Determined by FlowRouter based on routing rules
- âœ… Stored in contextvars
- âœ… Included in all error records
- âœ… Used for flow-specific tracking and debugging

---

## Error Handling Patterns

### Pattern 1: Service Layer Error Handling
```python
async def service_function(data: Dict) -> Dict[str, Any]:
    tenant_id = data.get("tenant_id")
    request_id = get_request_id()
    
    try:
        # Business logic
        result = process_data(data)
        return {"status": "success", "data": result}
        
    except Exception as exc:
        tb = traceback.format_exc()
        log("ERROR", f"Service error: {exc}", module="service_name", 
            tenant_id=tenant_id)
        await audit_event("service_error", tenant_id, None, 
                         {"error": str(exc), "traceback": tb}, 
                         request_id=request_id)
        await send_slack_alert(f"Service error: {exc}", 
                              context={"tenant_id": tenant_id, "traceback": tb}, 
                              severity="CRITICAL", 
                              module="service_name", 
                              request_id=request_id)
        return {"status": "error", "message": str(exc)}
```

### Pattern 2: Integration Layer Error Handling
```python
async def integration_function(params: Dict) -> Dict[str, Any]:
    try:
        # External API call
        response = await external_api.call(params)
        return response
        
    except Exception as exc:
        await record_error(
            component="integration_name",
            function="integration_function",
            message=f"Integration failed: {exc}",
            severity="ERROR",
            tenant_id=params.get("tenant_id"),
            stacktrace=traceback.format_exc(),
            details={"params": params}
        )
        raise  # Propagate for caller handling
```

### Pattern 3: Scheduler Job Error Handling
```python
async def scheduled_job():
    items = []
    try:
        # Fetch items to process
        async with get_async_session() as db:
            items = await db.execute(query)
    except Exception as exc:
        log("ERROR", f"Failed to fetch items: {exc}", module="scheduler")
        await send_slack_alert(f"Job fetch failed: {exc}", 
                              context={"traceback": traceback.format_exc()}, 
                              severity="CRITICAL", 
                              module="scheduler")
        return
    
    # Process each item individually
    for item in items:
        try:
            await process_item(item)
        except Exception as exc:
            tb = traceback.format_exc()
            log("ERROR", f"Item processing error: {exc}", module="scheduler", 
                tenant_id=item.tenant_id)
            await audit_event("job_item_error", item.tenant_id, None, 
                            {"item_id": str(item.id), "error": str(exc), "traceback": tb})
            # Continue with next item
```

---

## Recommendations & Improvements

### ðŸŸ¡ Minor Improvements

#### 1. Standardize Severity Levels
**Current:** Some places use string literals "ERROR", "CRITICAL"  
**Recommendation:** Define enum for type safety

```python
# app/monitoring/errors.py
from enum import Enum

class ErrorSeverity(str, Enum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

# Update all calls to use ErrorSeverity.ERROR instead of "ERROR"
await record_error(
    component="service",
    message="Error occurred",
    severity=ErrorSeverity.ERROR  # Type-safe
)
```

#### 2. Add Correlation IDs
**Current:** Uses request_id, but no cross-request correlation  
**Recommendation:** Add optional correlation_id for tracking related events

```python
class ErrorLog(Base):
    # ... existing fields
    correlation_id = Column(String, nullable=True, index=True)  # Track related errors
```

**Use case:** Track an entire multi-step flow (webhook â†’ normalization â†’ processing â†’ notification)

#### 3. Add Error Metrics
**Current:** No aggregated metrics  
**Recommendation:** Add Prometheus metrics

```python
# app/monitoring/metrics.py
from prometheus_client import Counter, Histogram

error_counter = Counter('errors_total', 'Total errors', 
                       ['component', 'severity', 'tenant_id'])
error_processing_time = Histogram('error_processing_seconds', 
                                 'Error processing time')

# In record_error():
error_counter.labels(
    component=component, 
    severity=severity, 
    tenant_id=tenant_id or "unknown"
).inc()
```

#### 4. Enhance Admin API with Search
**Current:** Admin API only supports list with tenant filter  
**Recommendation:** Add flexible search endpoint

```python
@router.get("/admin/errors/search")
async def search_errors(
    request_id: Optional[str] = None,
    component: Optional[str] = None,
    severity: Optional[str] = None,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    tenant_id: Optional[str] = None,
    limit: int = 100,
    db: AsyncSession = Depends(get_async_session)
):
    """
    Advanced error search with multiple filters.
    
    Query Parameters:
    - request_id: Find errors by request ID
    - component: Filter by component name
    - severity: Filter by severity level
    - start_date: Filter errors after this date
    - end_date: Filter errors before this date
    - tenant_id: Filter by tenant
    - limit: Max results to return
    """
    query = select(ErrorLog)
    
    if request_id:
        query = query.filter(ErrorLog.request_id == request_id)
    if component:
        query = query.filter(ErrorLog.component == component)
    if severity:
        query = query.filter(ErrorLog.severity == severity)
    if start_date:
        query = query.filter(ErrorLog.created_at >= start_date)
    if end_date:
        query = query.filter(ErrorLog.created_at <= end_date)
    if tenant_id:
        query = query.filter(ErrorLog.tenant_id == tenant_id)
    
    query = query.limit(limit).order_by(ErrorLog.created_at.desc())
    result = await db.execute(query)
    errors = result.scalars().all()
    
    return {"errors": [serialize_error(e) for e in errors]}
```

#### 5. Add Error Retry Mechanism
**Current:** Errors logged but no automatic retry for transient failures  
**Recommendation:** Add failed_events table with retry logic

```python
# app/db/models.py
class FailedEvent(Base):
    __tablename__ = "failed_events"
    
    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid4)
    event_id = Column(UUID(as_uuid=True), ForeignKey("processed_events.id"))
    retry_count = Column(Integer, default=0)
    max_retries = Column(Integer, default=3)
    next_retry_at = Column(DateTime)
    error_log_id = Column(UUID(as_uuid=True), ForeignKey("error_logs.id"))
    last_error = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

# Scheduler job for retry
async def retry_failed_events():
    """Scheduler job to retry failed events with exponential backoff"""
    async with get_async_session() as db:
        query = select(FailedEvent).filter(
            FailedEvent.retry_count < FailedEvent.max_retries,
            FailedEvent.next_retry_at <= datetime.utcnow()
        )
        result = await db.execute(query)
        failed_events = result.scalars().all()
        
        for fe in failed_events:
            try:
                # Retry processing
                await process_event(fe.event_id)
                # Delete from failed_events on success
                await db.delete(fe)
            except Exception as exc:
                # Update retry info
                fe.retry_count += 1
                fe.last_error = str(exc)
                fe.next_retry_at = datetime.utcnow() + timedelta(
                    minutes=2 ** fe.retry_count  # Exponential backoff: 2, 4, 8 minutes
                )
        
        await db.commit()
```

#### 6. Add Error Aggregation Dashboard Data
**Current:** Individual error records only  
**Recommendation:** Add aggregation endpoints for dashboard visualization

```python
@router.get("/admin/errors/stats")
async def error_statistics(
    tenant_id: Optional[str] = None,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    db: AsyncSession = Depends(get_async_session)
):
    """
    Get error statistics for dashboard visualization.
    
    Returns:
    - total_errors: Total error count
    - by_severity: Error count grouped by severity
    - by_component: Error count grouped by component
    - by_tenant: Error count grouped by tenant
    - timeline: Error count over time (hourly/daily buckets)
    """
    query = select(ErrorLog)
    
    if tenant_id:
        query = query.filter(ErrorLog.tenant_id == tenant_id)
    if start_date:
        query = query.filter(ErrorLog.created_at >= start_date)
    if end_date:
        query = query.filter(ErrorLog.created_at <= end_date)
    
    # Execute aggregations
    # ... (use SQLAlchemy group_by, func.count, etc.)
    
    return {
        "total_errors": total_count,
        "by_severity": {...},
        "by_component": {...},
        "by_tenant": {...},
        "timeline": [...]
    }
```

---

## Testing Error Handling

### Unit Test Example
```python
# tests/unit/test_error_handling.py
import pytest
from app.monitoring.errors import record_error
from app.db.repositories.error_repository import ErrorRepository

@pytest.mark.asyncio
async def test_record_error_creates_db_entry(db_session):
    """Test that record_error creates an ErrorLog entry"""
    await record_error(
        component="test_component",
        message="Test error message",
        severity="ERROR",
        tenant_id="test_tenant",
        function="test_function",
        stacktrace="Test stacktrace"
    )
    
    repo = ErrorRepository(db_session)
    errors = await repo.list_by_tenant("test_tenant")
    
    assert len(errors) == 1
    assert errors[0].component == "test_component"
    assert errors[0].message == "Test error message"
    assert errors[0].severity == "ERROR"

@pytest.mark.asyncio
async def test_record_error_fail_safe(db_session, monkeypatch):
    """Test that record_error doesn't crash on DB failure"""
    def mock_error(*args, **kwargs):
        raise Exception("DB connection failed")
    
    monkeypatch.setattr("app.db.session.get_async_session", mock_error)
    
    # Should not raise exception
    await record_error(
        component="test_component",
        message="Test error",
        severity="ERROR"
    )
```

### Integration Test Example
```python
# tests/integration/test_error_flow.py
@pytest.mark.asyncio
async def test_service_error_logged_and_alerted(
    client, 
    db_session, 
    mock_slack_webhook
):
    """Test that service errors are logged, persisted, and alerted"""
    # Trigger error in preventivi service
    response = await client.post(
        "/webhooks/gmail",
        json={"invalid": "payload"}
    )
    
    # Verify error was persisted
    repo = ErrorRepository(db_session)
    errors = await repo.list_all(limit=10)
    assert len(errors) > 0
    
    # Verify Slack alert was sent
    assert mock_slack_webhook.called
    alert_payload = mock_slack_webhook.call_args[1]["json"]
    assert "ERROR" in alert_payload["text"] or "CRITICAL" in alert_payload["text"]
```

---

## Monitoring & Alerting Setup

### Log Aggregation
**Recommended Tools:**
- ELK Stack (Elasticsearch, Logstash, Kibana)
- Splunk
- AWS CloudWatch Logs
- Google Cloud Logging
- Datadog

**Example Kibana Query:**
```json
{
  "query": {
    "bool": {
      "must": [
        {"match": {"level": "ERROR"}},
        {"match": {"tenant_id": "acme_corp"}},
        {"range": {"timestamp": {"gte": "now-1h"}}}
      ]
    }
  }
}
```

### Slack Alerting
**Configuration:**
1. Create Slack incoming webhook: https://api.slack.com/messaging/webhooks
2. Set environment variable: `SLACK_WEBHOOK_URL=https://hooks.slack.com/services/...`
3. Alerts automatically sent for ERROR/CRITICAL severity

**Alert Format:**
```
ðŸš¨ [CRITICAL] preventivi_service: Preventivi processing error: Division by zero

*Module:* preventivi_service
*Request ID:* 123e4567-e89b-12d3-a456-426614174000

Traceback:
```
File "app/core/preventivi_service.py", line 85, in process_normalized_event
  result = calculate_total(quote_data)
File "app/core/preventivi_service.py", line 120, in calculate_total
  return total / quantity
ZeroDivisionError: division by zero
```
```

### Database Monitoring
**Queries for Monitoring:**

1. **Error count by severity (last 24h):**
```sql
SELECT severity, COUNT(*) as count
FROM error_logs
WHERE created_at >= NOW() - INTERVAL '24 hours'
GROUP BY severity
ORDER BY count DESC;
```

2. **Top error components:**
```sql
SELECT component, COUNT(*) as count
FROM error_logs
WHERE created_at >= NOW() - INTERVAL '7 days'
GROUP BY component
ORDER BY count DESC
LIMIT 10;
```

3. **Errors by tenant:**
```sql
SELECT tenant_id, COUNT(*) as count
FROM error_logs
WHERE created_at >= NOW() - INTERVAL '24 hours'
  AND tenant_id IS NOT NULL
GROUP BY tenant_id
ORDER BY count DESC;
```

4. **Recent critical errors:**
```sql
SELECT created_at, component, message, stacktrace
FROM error_logs
WHERE severity = 'CRITICAL'
  AND created_at >= NOW() - INTERVAL '1 hour'
ORDER BY created_at DESC;
```

---

## Summary Checklist

| Component | Status | Notes |
|-----------|--------|-------|
| **Global Exception Handler** | âœ… PASS | Comprehensive, includes logging/audit/Slack |
| **MonitoringCenter Logger** | âœ… PASS | Structured JSON, context injection, UUID support |
| **Error/Audit DB Storage** | âœ… PASS | Separate ErrorLog table, proper schema |
| **record_error() Helper** | âœ… PASS | Centralized, reusable, fail-safe |
| **Slack Alerts** | âœ… PASS | Optional, fail-safe, includes stacktraces |
| **Integration Layers** | âœ… PASS | Gmail, WhatsApp, OneDrive all have proper handling |
| **Service Layers** | âœ… PASS | Preventivi service has comprehensive error handling |
| **Scheduler Jobs** | âœ… PASS | All 3 jobs have proper error handling |
| **Error Consultation** | âœ… PASS | Admin API, ErrorRepository, logs, Slack |
| **Context Propagation** | âœ… PASS | request_id, tenant_id, flow_id tracked everywhere |

---

## Conclusion

### Overall Grade: âœ… EXCELLENT (95/100)

The error handling system is **production-ready** and follows industry best practices:

**âœ… Strengths:**
- Comprehensive error handling across all architectural layers
- Proper separation of concerns (ErrorLog vs AuditLog)
- Fail-safe design (monitoring never crashes pipeline)
- Rich context propagation (request_id, tenant_id, flow_id)
- Multiple consultation mechanisms (API, DB, logs, Slack)
- Structured logging for easy analysis and aggregation
- Optional Slack alerts for real-time visibility
- Individual error handling in batch operations (jobs)

**ðŸŸ¡ Minor Improvements:**
- Standardize severity levels with enum (type safety)
- Add correlation IDs for cross-request tracking
- Add error metrics (Prometheus) for dashboard visualization
- Implement error retry mechanism for transient failures
- Enhance admin API with advanced search capabilities
- Add error aggregation endpoints for statistics

**Recommendation:** 
The system is ready for production deployment. Consider implementing the minor improvements incrementally post-launch based on operational experience and monitoring insights.

---

## Quick Reference

### Recording an Error
```python
from app.monitoring.errors import record_error

await record_error(
    component="my_component",
    function="my_function",
    message="Error description",
    severity="ERROR",  # or "CRITICAL"
    tenant_id=tenant_id,
    flow_id=flow_id,
    stacktrace=traceback.format_exc(),
    details={"extra": "context"}
)
```

### Querying Errors via Admin API
```bash
# List all errors
curl http://localhost:8000/admin/errors

# List errors for specific tenant
curl http://localhost:8000/admin/errors?tenant_id=acme_corp

# Paginate results
curl http://localhost:8000/admin/errors?limit=50&offset=100
```

### Querying Errors via Database
```python
from app.db.repositories.error_repository import ErrorRepository

async with get_async_session() as db:
    repo = ErrorRepository(db)
    
    # Get errors for tenant
    errors = await repo.list_by_tenant("acme_corp", limit=100)
    
    # Get all errors
    all_errors = await repo.list_all(limit=100, offset=0)
```

### Searching Logs
```bash
# Using jq for JSON logs
cat logs.json | jq 'select(.level=="ERROR" and .tenant_id=="acme_corp")'

# Search by request_id
cat logs.json | jq 'select(.request_id=="123e4567-e89b-12d3-a456-426614174000")'

# Count errors by component
cat logs.json | jq -s 'group_by(.module) | map({component: .[0].module, count: length})'
```

---

**Document Version:** 1.0  
**Last Updated:** 11 December 2025  
**Status:** Production Ready âœ…
